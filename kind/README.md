# Introduction

There are some tools available in kind/aliases suited for various development purposes. See the file for an accurate list, but here follows a description.

## Prerequisites

Copy kind/.secrets.sample and insert a custom CA and key pair (for .env and .secrets file you have to remove line breaks...).

## Usage

`. kind/aliases`. E.g.: `echo '. kind/aliases' >> ~/.zshrc`.

[See below](#commands) for commands.

# Tools used with relevant documentation
Some binaries may undoubtedly not be present on your host OS. `brew install <bin>` is the preferred way of installing those, and the tools used are verified to be present in `brew`.

- https://kind.sigs.k8s.io/
- https://coredns.io/plugins/k8s_external/
- https://metallb.universe.tf/configuration/
- https://docs.docker.com/network/
- https://github.com/nektos/act
- https://github.com/rhysd/actionlint

# Files

## `.gitignore`

We've added the following to `.gitignore`:

```
workflow/
kind/kubeconfig
kind/.secrets
```

In the `kint` command we bind the CWD (`-b|--bind`). This has some obvious advantages, among which for instance one can debug the values repository `tests/kind` on the fly. It is also necessary for 3-way Docker in docker. 

### `workflow/`

Thus there are some static files in this repository as a result, which would usually be present in the container image and not on one's host. `workflow/` files are the actually ran commands of each step in a workflow's job before variable evaluation. It is useful for inspection.

### `kind/kubeconfig`

Is available to log into the created `kint` cluster.

## `kind/act.json`

Is used to mimic the Github Actions `github` context.

## `kind/Dockerfile`

The decision has been made to provide the k8s environment stand-alone, to mimic that the k8s environment is separated/isolated from that which is run as integration test against it. The consideration has been made to insert the KinD dependencies as charts in the rest of our repo, but by not having a separate development process, one cannot always assume that the build is ran against an agnostic environment. With public cloud deployments, for instance, we also don't concern ourselves about the provided k8s environment.

This image should be pushed to `otomi/kind`.

## `.github/workflows/main.yml#.integration`

The job should be self-explanatory for the most part, but here are some quirks:

- `${{ !github.event.act }}`: in combination with `kind/act.json`, it reads the contents of the `act` variable in `act.json` which is set to false. This means we can set some feature flags whether we want some steps to run locally, remotely, or both. We don't need/want to set `globalPullSecret` locally, for instance.
- `docker network create --driver bridge kind`: we create a user-defined Docker network named "kind" because it means we can run KinD in an isolated network.

## `kind/start-kind.sh`

This script is the entrypoint for the otomi/kind image. It creates a cluster, configures MetalLB and the CoreDNS k8s_external dependency for intra cluster host name resolution. 

These environment variables are open for modification:
- `KIND_EXPERIMENTAL_DOCKER_NETWORK` (default: kind): connect KinD to a Docker network. The use case does become clear when you have to dynamically assign a Docker network, such as in the official Github Actions runner, where you only have access to `GITHUB_NETWORK`. E.g., you can set `KIND_EXPERIMENTAL_DOCKER_NETWORK: ${{ env.GITHUB_NETWORK }}`. Locally it also makes sense to provide network isolation.
- `DOMAIN_SUFFIX` (default: kind.local): Can change it later if you want to set a custom domain suffix.

## `kind/kind.yaml`

Configures KinD, see the official documentation. 

# Commands

## `kint`

Short for Kubernetes INTegration test (trademark pending), uses `nektos/act` to run Github Actions locally. It presently only targets the `integration` job (`--job integration`). The advantage is that one doesn't have to check Github Actions logging for output, and it makes the development cycle shorter for obvious reasons.

For a full list of options, run `act`. 

Other noteworthy options:
- `--platform` targets the container image, by default `ubuntu:act-20.04`. It mimics most tools in the official Github Actions runner, but you might discover that some binaries are not available. Check https://github.com/catthehacker/docker_images for more images, or consider building your own.

## `kintd`

It mounts a `kubeconfig` file, generated by the `kint` alias, into a `k9s` container. The developer can immediately start debugging the cluster as it boots.

## `kintk`

Basically `kubectl` but connected to the `kind` docker network.

## `kintnet`

Run a container image with network debugging tools, connected to the `kind` network

## `klint`

Sometimes it is useful to lint the Github Actions file instead of committing and browsing to Github. 

## `kintoto`

Run otomi commands against the cluster.

## `kintprep`

Prepares an otomi/core image locally for your feature branch to use for locally running Github Actions with `kint`. For more info, see [Building an otomi/core image](#building-an-otomicore-image).

# Caveats

## False negatives in the `integration` step

As observed from our logs/direct observation, there are occurrences of false negatives in running a stack against KinD. We observed the following errors that have occurred randomly:

- `errImagePull: dial tcp: lookup registry-1.docker.io: Temporary failure in name resolution`
- ...

It will throw an error that the build has failed, while it might be a perfectly fine configuration. The most common solution is to try again after restarting or after a period. 

## Building an otomi/core image

Running `kint` locally assumes you have an image on your machine named `otomi/core:$NAME_OF_YOUR_BRANCH` available. Either build the most recent one or pull it from Dockerhub (if is already pushed). 

Consequently, you could potentially be running an outdated image, so please be aware. This can happen if you just checked out a new branch and an image with the branch name does not exist.

It's useful this way because if you compare build times, obviously the following holds true if you compare Docker time: building > pulling > exists. So you don't have to wait for anything to start debugging the integration test locally.