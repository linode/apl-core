{{- $v := .Values }}
{{- $c := $v.apps }}
{{- $k := $v.apps.keycloak }}
{{- $o := $v | get "oidc" dict }}
{{- $a := $v.apps | get "alertmanager" }}
{{- $g := $v.apps | get "grafana" }}
{{- $p := $v.apps | get "prometheus" }}
{{- $hasKeycloak := $k.enabled }}
{{- $appsDomain := printf "apps.%s" $v.cluster.domainSuffix }}
{{- $grafanaDomain := printf "grafana.%s" $v.cluster.domainSuffix }}
{{- $slackTpl := tpl (readFile "../../helmfile.d/snippets/alertmanager/slack.gotmpl") $v | toString }}
{{- $opsgenieTpl := tpl (readFile "../../helmfile.d/snippets/alertmanager/opsgenie.gotmpl") $v | toString }}
# {{- $grafanaIni := tpl (readFile "../../helmfile.d/snippets/grafana.gotmpl") (dict "keycloakBase" $v._derived.oidcBaseUrlBackchannel "untrustedCA" $v._derived.untrustedCA "keycloak" ($k | get "idp")) | toString }}
{{- $grafanaIni := tpl (readFile "../../helmfile.d/snippets/grafana.gotmpl") (dict "keycloakBase" $v._derived.oidcBaseUrl "untrustedCA" $v._derived.untrustedCA "keycloak" ($k | get "idp")) | toString }}
{{- $hasServices := false }}
{{- range $teamId, $team := $v.teamConfig }}
  {{- if gt (len ($team | get "services" list)) 0 }}{{ $hasServices = true }}{{ end }}
{{- end }}
nameOverride: po
fullnameOverride: po
{{- with .Values.otomi | get "globalPullSecret" nil }}
global:
  imagePullSecrets:
    - name: otomi-pullsecret-global
{{- end }}
prometheusOperator:
  resources:
    limits:
      cpu: 400m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  admissionWebhooks:
    enabled: true
    certManager:
      enabled: true
      issuerRef:
        name: custom-ca
        kind: ClusterIssuer
  priorityClassName: otomi-critical
commonLabels:
  prometheus: system
prometheus:
  enabled: {{ $p.enabled }}
  prometheusSpec:
    {{- range $selType := list "podMonitor" "probe" "rule" "serviceMonitor" }}
    {{ $selType }}NamespaceSelector:
      matchExpressions:
        - key: name
          operator: NotIn
          values:
            {{- range $teamId, $_ := $v.teamConfig }}
            - team-{{ $teamId }}
            {{- end }}
    {{ $selType }}Selector:
      matchLabels:
        prometheus: system
    {{- end }}
    replicas: {{ $p.replicas }}
    scrapeInterval: {{ $p.scrapeInterval }}
    podAntiAffinity: hard
    image:
      {{- with $g | get "image.prometheus.tag" nil }}
      tag: {{ . }}
      {{- end }}
      pullPolicy: {{ $g | get "image.prometheus.pullPolicy" "IfNotPresent" }}
    podMetadata:
      annotations:
        sidecar.istio.io/inject: "true"
    resources:
      {{- with $p | get "prometheus.resources" nil }}
        {{- toYaml . | nindent 6 }}
      {{- else }}
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: '3'
        memory: 3Gi
      {{- end }}
    priorityClassName: otomi-critical
    externalLabels:
      cluster: {{ printf "%s/%s/%s" $v.cluster.owner $v.cluster.provider $v.cluster.name | lower | quote }}
    retention: {{ $p | get "retention" "1d" }}
    retentionSize: {{ $p | get "retentionSize" }}
    storageSpec:
      volumeClaimTemplate:
        spec:
          {{- if $v._derived.supportedCloud }}
          storageClassName: fast
          {{- end }}
          resources:
            requests:
              storage: {{ $p | get "storageSize" }}
    enableAdminAPI: true
    externalUrl: https://{{ $appsDomain }}/prometheus
{{- if and (not $v.otomi.isMultitenant) $hasServices }}
    additionalScrapeConfigs:
  {{- range $teamId, $team := $v.teamConfig }}
    {{- $teamServices := ($team | get "services" list) }}
    {{- $domain := printf "team-%s.%s" $teamId $v.cluster.domainSuffix }}
    {{- tpl (readFile "../../helmfile.d/snippets/blackbox-targets.gotmpl") (dict "teamId" $teamId "services" $teamServices "domain" $domain) | nindent 6 }}
  {{- end }}
{{- end }}
  additionalPodMonitors:
    {{- range $m := (tpl (readFile "pod-monitors.gotmpl") $v | fromYaml) | get "additionalPodMonitors" }}
    - {{- toYaml $m | nindent 6 }}
      additionalLabels:
        prometheus: system
    {{- end }}
  additionalServiceMonitors:
    {{- range $m := (tpl (readFile "service-monitors.gotmpl") $v | fromYaml) | get "additionalServiceMonitors" }}
    - {{- toYaml $m | nindent 6 }}
      additionalLabels:
        prometheus: system
    {{- end }}
{{ if or (not $v.otomi.isMultitenant) (eq $v.cluster.provider "aws") }}  
additionalPrometheusRules:
  {{- if not $v.otomi.isMultitenant }}
  - name: blackbox
    {{- readFile "rules/blackbox.yaml" | nindent 4 }}
  {{- end }}
  {{- if eq $v.cluster.provider "aws" }}
  - name: cluster-autoscaler
    {{- readFile "rules/cluster-autoscaler.yaml" | nindent 4 }}
  {{- end }}
{{- end }}
alertmanager:
  enabled: {{ $a.enabled }}
  alertmanagerSpec:
    podMetadata:
      annotations:
        sidecar.istio.io/inject: "true"
      labels:
        prometheus: system
    image:
       {{- with $a | get "image.tag" nil }}
      tag: {{ . }}
      {{- end }}
      pullPolicy: {{ $a | get "image.pullPolicy" "IfNotPresent" }}
    priorityClassName: otomi-critical
    resources:
      {{- with $a | get "resources" nil }}
        {{- toYaml . | nindent 4 }}
      {{- else }}
      limits:
        cpu: 500m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi
      {{- end }}
    externalUrl: https://{{ $appsDomain }}/alertmanager
  config: {{- tpl (readFile "../../helmfile.d/snippets/alertmanager.gotmpl") (dict "instance" $v "root" $v "slackTpl" $slackTpl "opsgenieTpl" $opsgenieTpl) | nindent 4 }}
grafana:
  enabled: {{ $g.enabled }}
  nameOverride: po-grafana
  fullnameOverride: po-grafana
  namespaceOverride: grafana
  defaultDashboardsTimezone: browser
  downloadDashboards:
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi
  image:
    {{- with $g | get "image.tag" nil }}
    tag: {{ . }}
    {{- end }}
    pullPolicy: {{ $g | get "image.pullPolicy" "IfNotPresent" }}
  resources:
    {{- with $g | get "resources" nil }}
      {{- toYaml . | nindent 4 }}
    {{- else }}
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
    {{- end }}
  sidecar:
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi
  serviceMonitor:
    # namespace: grafana
    labels:
      prometheus: system

  testFramework:
    enabled: false
    
  plugins:
    - grafana-piechart-panel
  additionalDataSources:
    - name: Loki
      editable: false
      type: loki
      access: proxy
      url: http://loki.monitoring:3100
      {{- if $v.otomi.isMultitenant }}
      url: http://loki.monitoring:3101
      basicAuth: true
      basicAuthUser: otomi-admin
      secureJsonData:
        basicAuthPassword: {{ $v.apps.loki.adminPassword }}
      {{- else }}
      url: http://loki.monitoring:3100
      {{- end }}
    {{- if $v | get "azure.monitor" nil }}
    - {{- tpl (readFile "../../helmfile.d/snippets/azure-monitor.gotmpl") $v.azure.monitor | toString | nindent 6 }}
    {{- end }}
  adminPassword: {{ $g | get "adminPassword" $v.otomi.adminPassword }}
  grafana.ini: {{- $grafanaIni | nindent 4 }}
    server:
      root_url: https://{{ $grafanaDomain }}
      # serve_from_sub_path: true

kube-state-metrics:
  prometheus:
    monitor:
      additionalLabels:
        prometheus: system
  image:
    {{- with $p | get "image.kube-state-metrics.tag" nil }}
    tag: {{  . }}
    {{- end }}
    pullPolicy: {{ $p | get "image.kube-state-metrics.pullPolicy" "IfNotPresent" }}
  resources:
    {{- with $p | get "resources.kube-state-metrics" nil }}
      {{- toYaml . | nindent 4 }}
    {{- else }}
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
    {{- end }}

prometheus-node-exporter:
  prometheus:
    monitor:
      additionalLabels:
        prometheus: system
  image:
    {{- with $p | get "image.node-exporter.tag" nil }}
    tag: {{ . }}
    {{- end }}
    pullPolicy: {{ $p | get "image.node-exporter.pullPolicy" "IfNotPresent" }}
  resources:
    {{- with $p | get "resources.prometheus-node-exporter" nil }}
      {{- toYaml . | nindent 4 }}
    {{- else }}
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi
    {{- end }}
  priorityClassName: otomi-critical
  podAnnotations:
    policy.otomi.io/ignore: psp-host-filesystem,psp-host-networking-ports,psp-host-security

kubeProxy:
  enabled: false
kubeScheduler:
  enabled: false
kubeControllerManager:
  enabled: flase
