{{- $v := .Environment.Values }}
{{- $c := $v.charts }}
{{- $k := $v.charts | get "keycloak" dict }}
{{- $p := $v.charts | get "prometheus-operator" dict }}
{{- $hasKeycloak := $k | get "enabled" true }}
{{- $realm := $k | get "realm" "master" }}
{{- $keycloakBase := printf "https://keycloak.%s/realms/%s" $v.cluster.domain $realm }}
{{- $o := $v.oidc }}
{{- $hasOIDC := or $hasKeycloak (hasKey $o "grafana") }}
{{- $appsDomain := printf "apps.%s" $v.cluster.domain }}
{{- $slackTpl := tpl (readFile "./slack-configs.gotmpl") $v | toString }}
nameOverride: po
fullnameOverride: po
coreDns:
  enabled: false
global:
  rbac:
    pspEnabled: false
{{- if $v.otomi.isManaged }}
kubeEtcd:
  enabled: false
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false
kubeProxy:
  enabled: false
{{- end }}
prometheusOperator:
  configReloaderCpu: 50m
  configReloaderMemory: 64Mi
  resources:
    limits:
      cpu: 400m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  admissionWebhooks:
    enabled: false
  tlsProxy:
    enabled: false
  createCustomResource: false # chart 8.2.0 version
  manageCrds: false # chart > 8.2.0
  kubeletService:
    enabled: true
  priorityClassName: "otomi-critical"
prometheus:
  prometheusSpec:
    resources:
      {{- if (hasKey $p "resources.prometheus") }}
        {{- $p.resources.prometheus | toYaml | nindent 6 }}
      {{- else }}
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi
      {{- end }}
    priorityClassName: "otomi-critical"
    externalLabels:
      cluster: {{ printf "%s/%s/%s" $v.customer.name $v.cluster.provider $v.cluster.name | lower | quote }}
    # podMetadata:
    #   annotations:
    #     sidecar.istio.io/inject: "false"
    portName: http-web
    storageSpec:
      volumeClaimTemplate:
        metadata:
          name: promdata
        spec:
          storageClassName: fast
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: {{ $p | get "prometheus.storageSize" "5Gi" }}
    enableAdminAPI: true
    externalUrl: https://{{ $appsDomain }}/prometheus
  additionalServiceMonitors: {{- readFile "service-monitors.yaml" | nindent 4 }}      
additionalPrometheusRules:
  - name: cluster-autoscaler
    {{- readFile "rules/cluster-autoscaler.yaml" | nindent 4 }}
alertmanager:
  alertmanagerSpec:
    priorityClassName: "otomi-critical"
    resources:
      requests:
        memory: 128Mi
        cpu: 100m
      limits:
        memory: 256Mi
        cpu: 500m
    # podMetadata:
    #   annotations:
    #     sidecar.istio.io/inject: "false"
    portName: http-web
    externalUrl: https://{{ $appsDomain }}/alertmanager
  config:
    {{- $hasSlack := eq ($p.alertmanager | get "receiver" "slack") "slack" }}
    global:
      slack_api_url: {{ $v.otomi.isRedkubesMonitored | ternary $v.redkubesSlackUrl ($p.alertmanager | get "slack.url" $v.redkubesSlackUrl) }}
    route:
      receiver: default
      group_by: [alertname]
      group_interval: {{ $p.alertmanager.groupInterval }}
      repeat_interval: {{ $p.alertmanager.repeatInterval }}
      routes:
        - match:
            alertname: Watchdog
          receiver: "null"
        - match:
            alertname: CPUThrottlingHigh
          receiver: "null"
        {{- if eq $v.cluster.provider "azure" }}
        - match:
            alertname: KubeAPILatencyHigh
          receiver: "null"
        {{- end }}
        # redkubes monitoring: only alerts that are for teams AND non-critical should go to the configured receiver
        # > so NO config here for admins
        - match:
            severity: critical
          receiver: critical
        {{- if $v.otomi.isRedkubesMonitored }}
          continue: true
        - match:
            severity: critical
          receiver: critical-redkubes
        {{- end }}
    receivers:
      - name: "null"
      {{- $channel := $p.alertmanager | get "slack.channel" "mon-otomi" }}
      {{- if or $v.otomi.isRedkubesMonitored $hasSlack }}
      - name: default
        slack_configs:
          - channel: "#{{ $channel }}"
            {{ $slackTpl | nindent 12 }}
      - name: critical
        slack_configs:
          - channel: "#{{ $channel }}-crit"
            {{ $slackTpl | nindent 12  }}
      {{- else }}
      - name: default
        webhook_configs:
          - url: "http://prometheus-msteams.monitoring.svc.cluster.local:2000/low_priority_channel"
            send_resolved: true
      - name: critical
        webhook_configs:
          - url: "http://prometheus-msteams.monitoring.svc.cluster.local:2000/high_priority_channel"
            send_resolved: true
      {{- end }}
      {{- if not $v.otomi.isRedkubesMonitored }}
      - name: critical-redkubes
        # sending criticals also to redkubes to be aware of customer issues
        slack_configs:
          - channel: "#{{ $channel }}-crit"
            api_url: {{ $v.redkubesSlackUrl }}
            {{ $slackTpl | nindent 12  }}
      {{- end }}
grafana:
  image:
    tag: 7.1.5
  resources:
    {{- if (hasKey $p "resources.grafana") }}
      {{- $p.resources.grafana | toYaml | nindent 4 }}
    {{- else }}
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 256Mi
      cpu: 500m
    {{- end }}
  nameOverride: po-grafana
  fullnameOverride: po-grafana
  rbac:
    pspUseAppArmor: false
    pspEnabled: false
  # podAnnotations:
  #   sidecar.istio.io/inject: "false"
  plugins:
    - grafana-piechart-panel
  additionalDataSources:
    - name: Loki
      editable: false
      type: loki
      access: proxy
      {{- if $v.otomi.isMultitenant }}
      url: http://loki:11811
      basicAuth: true
      basicAuthUser: admins
      basicAuthPassword: {{ $v.charts.loki.adminPassword }}
      {{- else }}
      url: http://loki:3100
      {{- end }}
    {{- with $v | getOrNil "azure.monitor" }}
    - name: Azure Monitor
      type: grafana-azure-monitor-datasource
      access: proxy
      jsonData:
        cloudName: azuremonitor
        subscriptionId: {{ $v.azure.subscriptionId }}
        tenantId: {{ $v.azure.tenantId }}
        clientId: {{ .clientId }}
        logAnalyticsTenantId: {{ . | get "logAnalyticsTenantId" $v.azure.tenantId }}
        logAnalyticsClientId: {{ . | get "logAnalyticsClientId" .clientId }}
        logAnalyticsDefaultWorkspace: {{ .logAnalyticsWorkspace }}
        appInsightsAppId: {{ . | get "appInsightsAppId" .clientId }}
        azureLogAnalyticsSameAs: true
        keepCookies: []
      secureJsonData:
        clientSecret: {{ .clientSecret }}
        logAnalyticsClientSecret: {{ . | get "logAnalyticsClientSecret" .clientSecret }}
        appInsightsAppSecret : {{ . | get "appInsightsAppSecret" .clientSecret }}
      version: 4
      editable: false
    {{- end }}
    {{- if $c.sitespeed.enabled }}
    - name: Graphite
      editable: false
      type: graphite
      access: proxy
      url: http://graphite:80
    {{- end }}
  adminPassword: {{ $p.grafana.adminPassword }}
  service:
    portName: http-service
  grafana.ini:
    "auth.anonymous":
      enabled: false
      # enabled: true
      org_role: Admin
      org_name: Main Org.
    "auth.generic_oauth":
      tls_skip_verify_insecure: {{ eq ($v.charts | get "cert-manager.stage") "staging" }}
      # enabled: false
      enabled: true
      name: OAuth
      org_role: Admin
      allow_sign_up: true
      # allow_sign_up: false
      oauth_auto_login: true # false = so we can login with admin / bladibla
      client_id: {{ $v.oidc.clientID }}
      client_secret: {{ $v.oidc.clientSecret }}
      scopes: openid
      auth_url: {{ $hasKeycloak | ternary (printf "%s/protocol/openid-connect/auth" $keycloakBase) ($o | getOrNil "grafana.authUrl") }}
      token_url: {{ $hasKeycloak | ternary (printf "%s/protocol/openid-connect/token" $keycloakBase) ($o | getOrNil "grafana.tokenUrl") }}
      api_url: {{ $hasKeycloak | ternary (printf "%s/protocol/openid-connect/userinfo" $keycloakBase) ($o | getOrNil "grafana.apiUrl") }}
      role_attribute_path: contains(groups[*], 'admin') && 'Admin' || contains(groups[*], 'team-admin') && 'Admin' || 'Editor'
    log:
      level: error
    server:
      root_url: https://{{ $appsDomain }}/grafana
    users:
      allow_sign_up: false
      auto_assign_org: true
      auto_assign_org_role: Viewer

kube-state-metrics:
  # image:
  #   tag: v1.8.0
  # collectors:
  #   networkpolicies: false
  #   mutatingwebhookconfigurations: false
  #   validatingwebhookconfigurations: false
  #   volumeattachments: false
  #   verticalpodautoscalers: false
  # podAnnotations:
  #   sidecar.istio.io/inject: "false"
  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 256Mi
      cpu: 400m
  podSecurityPolicy:
    enabled: false

prometheus-node-exporter:
  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 128Mi
      cpu: 500m
  rbac:
    pspEnabled: false
  priorityClassName: "otomi-critical"
