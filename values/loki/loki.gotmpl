{{- $v := .Values -}}
{{- $l := $v.apps | get "loki" }}
{{- $teamNames := "" }}
{{- range $name, $config := $v.teamConfig }}
{{- $teamNames = print "%s%s" $teamNames $name }}
{{- end }}
# enable tracing for debug, need install jaeger and specify right jaeger_agent_host

# @TODO:
image:
  tag: 2.4.1

tracing:
  jaegerAgentHost:

config:
  auth_enabled: {{ $v.otomi.isMultitenant }}
  limits_config:
    reject_old_samples_max_age: {{ $l | get "retention.duration" "24h" }}
  schema_config:
    configs:
    {{- if $l | get "v11StartDate" nil }}
    - from: '2018-04-15'
      store: boltdb
      object_store: filesystem
      schema: v9
      index:
        prefix: index_
        period: {{ $l | get "retention.period" "24h" }}
    {{- end }}
    - from: {{ $l | get "v11StartDate" "2021-05-12" }}
      store: boltdb-shipper
      object_store: {{ $l | get "storage.storageType" "filesystem" }}
      schema: v11
      index:
        prefix: index_
        period: 24h
  storage_config:
    boltdb_shipper:
      shared_store: {{ $l | get "storage.storageType" "filesystem" }}
    filesystem:
      directory: /data/loki/chunks
    {{- if ne ($l | get "storage.storageType" "filesystem") "filesystem" }}
    {{ $l.storage.storageType }}: {{- $l.storage | get $l.storage.storageType | toYaml | nindent 6 }}
    {{- end }}
  compactor:
    retention_enabled: true
    shared_store: {{ $l | get "storage.storageType" "filesystem" }}
  chunk_store_config:
    max_look_back_period: {{ $l | get "retention.period" "24h" }}
  table_manager:
    retention_deletes_enabled: true
    retention_period: {{ $l | get "retention.period" "24h" }}

networkPolicy:
  enabled: false

## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
## If you set enabled as "True", you need :
## - create a pv which above 10Gi and has same namespace with loki
## - keep storageClassName same with below setting
persistence:
  enabled: true
  accessModes:
  - ReadWriteOnce
  size: {{ $l | get "persistence.size" "20Gi" }}
  {{- if $v._derived.supportedCloud }}
  storageClassName: fast
  {{- end }}

## Assign a PriorityClassName to pods if set
priorityClassName: otomi-critical

resources:
  {{- with $l | get "resources" nil }}
    {{- toYaml . | nindent 2 }}
  {{- else }}
  limits:
    cpu: 800m
    memory: 512Mi
  requests:
    cpu: 100m
    memory: 128Mi
  {{- end }}

fullnameOverride: loki

promtail:
  serviceName: promtail

extraContainers:
{{- if $v.otomi.isMultitenant }}
- name: reverse-proxy
  image: k8spin/loki-multi-tenant-proxy:v1.0.0
  args:
    - "run"
    - "--port=3101"
    - "--loki-server=http://localhost:3100"
    - "--auth-config=/etc/reverse-proxy-conf/authn.yaml"
  ports:
    - name: http
      containerPort: 3101
      protocol: TCP
  resources:
    limits:
      cpu: 250m
      memory: 200Mi
    requests:
      cpu: 50m
      memory: 40Mi
  volumeMounts:
    - name: reverse-proxy-auth-config
      mountPath: /etc/reverse-proxy-conf
{{- end }}


extraVolumes:
{{- if $v.otomi.isMultitenant }}
- name: reverse-proxy-auth-config
  secret:
    secretName: reverse-proxy-auth-config
{{- end }}
extraVolumeMounts: []

extraPorts:
{{- if $v.otomi.isMultitenant }}
- port: 3101
  protocol: TCP
  name: http
  targetPort: http
{{- end }}

serviceMonitor:
  enabled: true
  additionalLabels:
    prometheus: system

extraArgs:
  # log.level: debug

podAnnotations:
  sidecar.istio.io/inject: "false"
  {{- if $v.otomi.isMultitenant }}
  checksum/teams: {{ $teamNames | sha256sum }}
  {{- end }}  

{{- with .Values.otomi | get "globalPullSecret" nil }}
image:
  pullSecrets:
    - otomi-pullsecret-global
{{- end }}