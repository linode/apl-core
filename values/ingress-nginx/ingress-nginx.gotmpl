{{- $v := .Values }}
{{- $ingress := (index $v.ingress.classes (.Release.Labels.index | int)) }}
{{- $appName := printf "ingress-nginx-%s" $ingress.className }}
{{- $app := $v.apps | get $appName dict }}
{{/* appDefaults - each ingress-nginx Helm realse will use default values defined in defaults.yaml in app.ingress-nginx context*/}}
{{- $appDefaults := $v | get "apps.ingress-nginx" }}
{{- $n := deepCopy $appDefaults | merge $app }}
{{- $privateNetwork := eq "private" ($ingress | get "network" "public")  }}

nameOverride: {{ .Release.Name }}

controller:
  # @TODO:
  electionID: ingress-controller-leader-{{ $ingress.className }}
  ingressClassResource: 
    enabled: false
    controllerValue: k8s.io/{{ $ingress.className }}
  image:
    tag: {{ $n | get "image.tag" "v1.7.1" }}
    pullPolicy:  {{ $n | get "image.pullPolicy" "IfNotPresent" }}
  scope:
    enabled: true
    namespace: "istio-system"
  containerPort:
    http: 80
    https: 443
  admissionWebhooks:
    enabled: false
    patch:
      priorityClassName: otomi-critical
  useComponentLabel: true
  # set fixed allocation with limits same as requests
  resources:
    {{- if (hasKey $n "resources") }}
      {{- $n.resources | toYaml | nindent 4 }}
    {{- else }}
    limits:
        cpu: 2
        memory: 1.5Gi
    requests:
        cpu: 200m
        memory: 512Mi
    {{- end }}
  podAnnotations:
    policy.otomi.io/ignore: psp-privileged
  opentelemetry:
    enabled: {{ $n.tracing.enabled }}
    containerSecurityContext:
      runAsUser: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      allowPrivilegeEscalation: false
    resources:
      requests:
        cpu: 100m
        memory: 65Mi
      limits:
        cpu: 500m
        memory: 256Mi
  replicaCount: 2
  minAvailable: 1
  autoscaling:
    enabled: {{ $n.autoscaling.enabled }}
    minReplicas: {{ $n.autoscaling.minReplicas }}
    maxReplicas: {{ $n.autoscaling.maxReplicas }}
    targetCPUUtilizationPercentage: 60
    targetMemoryUtilizationPercentage: 75
{{- if eq $v.cluster.provider "azure" }}
  nodeSelector: 
    beta.kubernetes.io/os: linux
{{- end }}
  priorityClassName: otomi-critical
  extraArgs:
    v: 3
    enable-ssl-passthrough: true
  config:
    client-body-timeout: 5
    client-header-timeout: 5
    client-max-body-size: 0
    disable-ipv6: true
    enable-modsecurity: {{ $n.modsecurity.enabled }}
    enable-owasp-modsecurity-crs: {{ $n.modsecurity.owasp }}
    hsts: true
    http2-max-field-size: 64k
    http2-max-header-size: 128k
    large-client-header-buffers: 8 16k
    {{- if $n.modsecurity.enabled }}
    modsecurity-snippet: |
      SecRuleEngine {{ if $n.modsecurity.block }}On{{ else }}DetectionOnly{{ end }}
      SecAuditEngine RelevantOnly
      # SecRequestBodyAccess Off
      SecAuditLogParts ABDEFHIJZ
      SecAuditLogFormat JSON
      SecAuditLogType Serial
      SecAuditLog /dev/stdout
      # overrides for owasp go before inclusion of the defaults in nginx-modsecurity.conf
      SecRequestBodyLimit {{ $n | get "maxBodySizeBytes" "1073741824" }}
      SecRuleRemoveById 920350
      {{- if $n.modsecurity.owasp }}
      # Include /etc/nginx/owasp-modsecurity-crs/nginx-modsecurity.conf
      {{- end }}
    {{- end }}
    proxy-buffers-number: 8
    proxy-buffer-size: 16k
    proxy-body-size: {{ $n | get "maxBodySize" "1024m" }}
    # log-format-escape-json: true
    log-format-upstream: $proxy_protocol_addr - $remote_user [$time_local] $host "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_length $request_time [$proxy_upstream_name] [$proxy_alternative_upstream_name] $upstream_addr $upstream_response_length $upstream_response_time $upstream_status $req_id
    # log-format-upstream: '{"time":"$time_iso8601","remote_addr":"$proxy_protocol_addr","x_forward_for":"$proxy_add_x_forwarded_for","request_id":"$req_id","remote_user":"$remote_user","bytes_sent":$bytes_sent,"request_time":$request_time,"status":$status,"vhost":"$host","request_proto":"$server_protocol","path":"$uri","request_query":"$args","request_length":$request_length,"duration":$request_time,"method":"$request_method","http_referrer":"$http_referer","http_user_agent":"$http_user_agent"}'
    ssl-redirect: true
    use-forwarded-headers: false
    use-proxy-protocol: true
    enable-opentelemetry: {{ $n.tracing.enabled }}
    otel-sampler: AlwaysOn
    otel-sampler-ratio: {{ $n.tracing.samplingRatio }}
    otlp-collector-host: otel-collector-collector.otel.svc
    otlp-collector-port: 4317
    opentelemetry-config: "/etc/nginx/opentelemetry.toml"
    opentelemetry-operation-name: "HTTP $request_method $service_name $uri"
    opentelemetry-trust-incoming-span: "true"
    otel-max-queuesize: "2048"
    otel-schedule-delay-millis: "5000"
    otel-max-export-batch-size: "512"
    otel-service-name: "nginx-{{ $ingress.className }}"
    otel-sampler-parent-based: "true"
  stats:
    enabled: true
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels: # needed to be picked up by our one and only prometheus-operator:
        prometheus: system
  service:
    externalTrafficPolicy: Local
    type: LoadBalancer
    omitClusterIP: true
    {{- with $ingress | get "loadBalancerIP" nil }}
    loadBalancerIP: {{ . }}
    {{- end }}
    annotations:
      dummy: 'true'
      {{- with $ingress | get "service.annotations" nil }}
      {{ toYaml . | indent 6 }}
      {{- end }}
      {{- if eq $v.cluster.provider "azure" }}
      {{- with $ingress | get "loadBalancerRG" nil }}
      service.beta.kubernetes.io/azure-load-balancer-resource-group: {{ . }}
      {{- end }}
      {{- if $privateNetwork }}
      service.beta.kubernetes.io/azure-load-balancer-internal: "true"
        {{- with $ingress | get "loadBalancerIP" nil }}
      service.beta.kubernetes.io/azure-load-balancer-internal-subnet: "{{ . }}"
        {{- end }}
      {{- end }}
      {{- end }} {{/* if eq $v.cluster.provider "azure" */}}
      {{- if eq $v.cluster.provider "aws" }}
      service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
      {{- if $privateNetwork }}
      service.beta.kubernetes.io/aws-load-balancer-internal: true
      {{- end }}
      {{- if eq $v.cluster.provider "digitalocean" }}
      service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: true
      {{- end }}
      {{- end }} {{/* if eq $v.cluster.provider "aws" */}}
      {{- if eq $v.cluster.provider "google" }}
      {{- if $privateNetwork }}
      networking.gke.io/load-balancer-type: "Internal"
      {{- end }}
      {{- end }} {{/* if eq $v.cluster.provider "google" */}}
  publishService:
    enabled: true

defaultBackend:
  enabled: true
  useComponentLabel: true
  priorityClassName: otomi-critical
  resources:
    limits:
      cpu: 20m
      memory: 20Mi
    requests:
      cpu: 10m
      memory: 10Mi

  # podAnnotations:
  #   sidecar.istio.io/inject: "false"
{{- if eq $v.cluster.provider "azure" }}
  nodeSelector: 
    beta.kubernetes.io/os: linux
{{- end }}
  service:
    omitClusterIP: true
rbac:
  create: true

{{- with .Values.otomi | get "globalPullSecret" nil }}
imagePullSecrets:
  - name: otomi-pullsecret-global
{{- end }}
