{{- $v := .Values }}
{{- $h := $v.apps.harbor }}
{{- $db := $v.databases.harbor }}
{{- $obj := $v.obj.provider }}
{{- $harborDomain := printf "harbor.%s" $v.cluster.domainSuffix }}
{{- $notaryDomain := printf "notary.%s" $v.cluster.domainSuffix }}
{{- $harborSecretName := ($harborDomain | replace "." "-") }}
{{- $notarySecretName := ($notaryDomain | replace "." "-") }}
{{- $externalUrl := printf "https://%s" $harborDomain }}
{{- $tag := $h | get "image.tag" "v2.6.4" }}
{{/*The cloudNativeDb operator is responsible for creating the secret*/}}

externalURL: {{ $externalUrl }}
fullnameOverride: harbor
# logLevel - debug, info, warning, error or fatal
logLevel: warning

harborAdminPassword: {{ $h.adminPassword }}
nameOverride: harbor
secretKey: {{ $h | get "secretKey" nil }}

updateStrategy:
    type: Recreate

chartmuseum:
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}
  resources: {{- $h.resources.chartmuseum | toYaml | nindent 6 }}

core:
  secretName: harbor-token-service-ca
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}
  resources: {{- $h.resources.core | toYaml | nindent 4 }}
  secret: {{ $h | get "core.secret" nil }}
  # secretName: {{ $harborSecretName }}
  xsrfKey: {{ $h | get "core.xsrfKey" nil }}

database:
  type: external
  external:
    # erase default password value from the Harbor Helm chart
    password: null
    coreDatabase: {{ $db.coreDatabase }}
    username: harbor
    host: harbor-otomi-db-rw.harbor.svc.cluster.local
    existingSecret: harbor-otomi-db-app

exporter:
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}

expose:
  tls:
    enabled: false
  type: clusterIP

internalTLS:
  # we use istio with mTLS enabled for harbor namespace
  enabled: false

jobservice:
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}
  jobLoggers:
    - stdout
  resources: {{- $h.resources.jobservice | toYaml | nindent 4 }}
  secret: {{ $h | get "jobservice.secret" nil }}

metrics:
  serviceMonitor:
    enabled: true
    additionalLabels:
      prometheus: system

nginx:
  resources: {{- $h.resources.nginx | toYaml | nindent 4 }}

notary:
  server:
    priorityClassName: otomi-critical
    image:
      tag: {{ $tag }}
  signer:
    priorityClassName: otomi-critical
    image:
      tag: {{ $tag }}
  enabled: false
  secretName: {{ $notarySecretName }}

persistence:
  enabled: true
  persistentVolumeClaim:
    registry:
      # Use the existing PVC which must be created manually before bound,
      # and specify the 'subPath' if the PVC is shared with other components
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 10Gi
    chartmuseum:
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 5Gi
    jobservice:
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 1Gi
    # If external database is used, the following settings for database will
    # be ignored
    database:
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 1Gi
    # If external Redis is used, the following settings for Redis will
    # be ignored
    redis:
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 1Gi
    trivy:
      existingClaim: ''
      subPath: ''
      accessMode: ReadWriteOnce
      size: 5Gi   
  imageChartStorage:
    {{- if eq $obj.type "minioLocal" "linode" }}
    disableredirect: true
    type: s3
    s3:
      {{- if eq $obj.type "minioLocal" }}
      bucket: harbor
      region: us-east-1
      regionendpoint: http://minio.minio.svc.cluster.local:9000
      accesskey: otomi-admin                          
      secretkey: {{ $v.otomi.adminPassword }}                            
      secure: false
      v4auth: true
      {{- end }}
      {{- if eq $obj.type "linode" }}
      bucket: {{ $obj.linode.buckets.harbor }}
      regionendpoint: https://{{ $obj.linode.region }}.linodeobjects.com
      accesskey: {{ $obj.linode.accessKeyId }}               
      secretkey: {{ $obj.linode.secretAccessKey }}
      region: {{ $obj.linode.region }}
      encrypt: false
      secure: true
      v4auth: true
      # By bumping the `multipartcopythresholdsize` option to 5GiB (up from its default
      # of 32Mib), Distribution will only initiate an asynchronous server-side COPY
      # operation once the blob is 5GiB in size. This effectively means there will be no
      # more 403 Forbidden errors from Object Storage unless a layer/blob is > 5GiB in size
      multipartcopythresholdsize: "5368709120"
      {{- end }}
    {{- else }}
    type: filesystem
    {{- end }}

postgresql:
  volumePermissions:
    enabled: true

portal:
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}
  resources: {{- $h.resources.portal | toYaml | nindent 4 }}

redis:
  internal:
    priorityClassName: otomi-critical
    image:
      tag: {{ $tag }}
  internal:
    resources: {{- $h.resources.redis | toYaml | nindent 6 }}

registry:
  priorityClassName: otomi-critical
  secret: {{ $h | get "registry.secret" nil }}

  registry:
    image:
      tag: {{ $tag }}
    resources: {{- $h.resources.registry | toYaml | nindent 6 }}
  controller:
    image:
      tag: {{ $tag }}
    resources: {{- $h.resources.registryController | toYaml | nindent 6 }}
  relativeurls: false
  credentials:
    username: {{ $h.registry.credentials.username }}
    password: {{ $h.registry.credentials.password }}
    htpasswdString: {{ $h.registry.credentials.htpasswd }}

trivy:
  priorityClassName: otomi-critical
  image:
    tag: {{ $tag }}
  resources: 
    {{- $h.resources.trivy | toYaml  | nindent 4 }}
  automountServiceAccountToken: true

{{- with .Values.otomi | get "globalPullSecret" nil }}
imagePullSecrets:
  - name: otomi-pullsecret-global
{{- end }}
